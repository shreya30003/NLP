{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd94857-6356-42c9-bcce-f5fbd8c88a69",
   "metadata": {},
   "source": [
    "### Rule Based Grammar Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad571f26-25df-4f94-9d54-dae006ec1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define grammar rules along with correction patterns\n",
    "grammar_rules = [\n",
    "    (r\"\\bI are\\b\", \"I am\"),\n",
    "    (r\"\\bHe do\\b\", \"He does\"),\n",
    "    (r\"\\bShe walks\\.\", \"She walks.\"),\n",
    "    (r\"\\bThe dog play\\b\", \"The dog plays\"),\n",
    "    (r\"\\bis very good\\.\", \"is very good.\"),\n",
    "    (r\"\\ba apple\\b\", \"an apple\"),\n",
    "    (r\"\\bthey is\\b\", \"they are\"),\n",
    "]\n",
    "\n",
    "def apply_grammar_rules(sentence):\n",
    "    # Apply grammar rules and corrections to the input sentence\n",
    "    corrected_sentence = sentence\n",
    "    errors = []\n",
    "    for rule, correction in grammar_rules:\n",
    "        corrected_sentence, count = re.subn(rule, correction, corrected_sentence)\n",
    "        if count > 0:\n",
    "            errors.append(f\"Corrected '{rule}' to '{correction}'\")\n",
    "    return corrected_sentence.strip(), errors\n",
    "\n",
    "def main():\n",
    "    # Get input text from the user\n",
    "    input_text = input(\"Enter the text to check and correct grammar: \")\n",
    "\n",
    "    # Tokenization (split input text into words)\n",
    "    tokens = input_text.split()\n",
    "\n",
    "    # Apply grammar rules to each token and generate corrected text\n",
    "    corrected_tokens = []\n",
    "    error_messages = []\n",
    "    for token in tokens:\n",
    "        corrected_token, errors = apply_grammar_rules(token)\n",
    "        corrected_tokens.append(corrected_token)\n",
    "        error_messages.extend(errors)\n",
    "\n",
    "    # Combine corrected tokens to form the corrected text\n",
    "    corrected_text = \" \".join(corrected_tokens)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nOriginal Text:\")\n",
    "    print(input_text)\n",
    "    print(\"\\nCorrected Text:\")\n",
    "    print(corrected_text)\n",
    "    if error_messages:\n",
    "        print(\"\\nGrammar Errors and Corrections:\")\n",
    "        for error in error_messages:\n",
    "            print(\"-\", error)\n",
    "    else:\n",
    "        print(\"\\nNo grammar errors found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616185b6-4dad-41c2-8314-54d3f4b2103f",
   "metadata": {},
   "source": [
    "### Syntax based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aab5ca1f-d51a-4f80-9072-6f2260375ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anjan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anjan\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a sentence to analyze:  a man reads the book\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Syntactic Analysis:\n",
      "              S               \n",
      "      ________|____            \n",
      "     |             VP         \n",
      "     |         ____|___        \n",
      "     NP       |        NP     \n",
      "  ___|___     |     ___|___    \n",
      "Det      N    V   Det      N  \n",
      " |       |    |    |       |   \n",
      " a      man reads the     book\n",
      "\n",
      "\n",
      "Grammar Errors:\n",
      "- Unknown word: 'the'\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG, ChartParser\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Define a context-free grammar (CFG) for simple English sentences\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "    S -> NP VP\n",
    "    NP -> Det N | Det N PP\n",
    "    VP -> V | V NP\n",
    "    PP -> P NP\n",
    "    \n",
    "    Det -> 'a' | 'an' | 'the'\n",
    "    N -> 'dog' | 'cat' | 'book' | 'man'\n",
    "    V -> 'runs' | 'eats' | 'reads'\n",
    "    P -> 'in' | 'on' | 'with'\n",
    "\"\"\")\n",
    "\n",
    "# Create a ChartParser based on the CFG\n",
    "parser = ChartParser(grammar)\n",
    "\n",
    "def analyze_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    parse_trees = list(parser.parse(tokens))  # Parse the sentence\n",
    "\n",
    "    if not parse_trees:\n",
    "        return \"Error: Unable to parse the sentence syntactically.\"\n",
    "\n",
    "    # Check for grammatical errors based on word meanings (synonyms)\n",
    "    errors = []\n",
    "    for word in tokens:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if not synsets:\n",
    "            errors.append(f\"Unknown word: '{word}'\")\n",
    "\n",
    "    return parse_trees[0], errors  # Return the syntactic tree and any detected errors\n",
    "\n",
    "def main():\n",
    "    # Get input sentence from the user\n",
    "    sentence = input(\"Enter a sentence to analyze: \")\n",
    "\n",
    "    # Perform syntactic analysis and error detection\n",
    "    parse_tree, errors = analyze_sentence(sentence)\n",
    "\n",
    "    # Display results\n",
    "    print(\"\\nSyntactic Analysis:\")\n",
    "    if isinstance(parse_tree, str):\n",
    "        print(parse_tree)  # Print parsing error message\n",
    "    else:\n",
    "        parse_tree.pretty_print()  # Print syntactic tree\n",
    "\n",
    "    if errors:\n",
    "        print(\"\\nGrammar Errors:\")\n",
    "        for error in errors:\n",
    "            print(\"-\", error)\n",
    "    else:\n",
    "        print(\"\\nNo grammar errors found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Download NLTK resources (if not already downloaded)\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    \n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c991cc7-7bc4-4945-ad96-abc700e1f1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
