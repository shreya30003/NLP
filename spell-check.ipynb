{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the list of candidate words using Trie structure for the misspelled word. Find the minimum edit distance and choose the correct word based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries and download the packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, brown, gutenberg\n",
    "from nltk.util import ngrams\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('brown')\n",
    "# nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Mr Patrick is our new principle\",\"The company excepted all the terms\",\"Please don't keep your dog on the lose\",\"The later is my best friend\",\"I need some stationary products for my craftwork\",\"The actor excepted the Oscar\",\"I will call you later in the evening\",\"Covid affects the lungs\",\"The council of the ministers were sworn in yesterday\",\"Robert too wants to accompany us to the park\",\"Mia will council me about choosing fashion as my career\", \"The bear at the zoo was very playful\",\"The sheep have a lot of fur that keeps them warm\",\"The hot spring is at the furthest corner of the street\",\"Can you advice me on how to study for exams\",\"The team will loose the match if they don't play well\",\"Can you go to the market for me\",\"The teachers asked the students to keep quite\", \"The heap of garbage should be cleaned immediately\",\"This is there house\",\"Mr Patrick is our new principal\",\"The company accepted all the terms\",\"Please don't keep your dog on the loose\",\"The latter is my best friend\",\"I need some stationery products for my craftwork\",\"The actor accepted the Oscar\",\"I will call you later in the evening\",\"Covid affects the lungs\",\"The council of the ministers were sworn in yesterday\",\"Robert too wants to accompany us to the park\",\"Mia will counsel me about choosing fashion as my career\", \"The bear at the zoo was very playful\",\"The sheep have a lot of fur that keeps them warm\",\"The hot spring is at the farthest corner of the street\",\"Can you advise me on how to study for exams\",\"The team will lose the match if they don't play wel.\",\"Can you go to the market for me\",\"The teachers asked the students to keep quiet\", \"The heap of garbage should be cleaned immediately\",\"This is their house\",\"Mr Patrick is our new principal\",\"The company accepted all the terms\",\"Please don't keep your dog on the loose\",\"The latter is my best friend\",\"I need some stationery products for my craftwork\",\"The actor accepted the Oscar\",\"I will call you later in the evening\",\"Covid affects the lungs\",\"The council of the ministers were sworn in yesterday\",\"Robert too wants to accompany us to the park\",\"Mia will counsel me about choosing fashion as my career\", \"The bear at the zoo was very playful\",\"The sheep have a lot of fur that keeps them warm\",\"The hot spring is at the farthest corner of the street\",\"Can you advise me on how to study for exams\",\"The team will lose the match if they don't play well\",\"Can you go to the market for me\",\"The teachers asked the students to keep quiet\", \"The heap of garbage should be cleaned immediately\",\"This is their house\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data and Data Preprocessing\n",
    "\n",
    "- Extending the original data\n",
    "- Spliting the data into words\n",
    "- Adding brown and gutenberg corpus to the data and creating the final corpus\n",
    "- Preprocessing Steps - lowercase, punctuation and stopwords removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.extend(data) # double the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = []\n",
    "for i in data:\n",
    "    sentence.append(i.split()) # split the data into seentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Mr', 'Patrick', 'is', 'our', 'new', 'principle'],\n",
       " ['The', 'company', 'excepted', 'all', 'the', 'terms'],\n",
       " ['Please', \"don't\", 'keep', 'your', 'dog', 'on', 'the', 'lose'],\n",
       " ['The', 'later', 'is', 'my', 'best', 'friend'],\n",
       " ['I', 'need', 'some', 'stationary', 'products', 'for', 'my', 'craftwork']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for i in data:\n",
    "    words.extend(i.split()) # split the data into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mr', 'Patrick', 'is', 'our', 'new']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:10] # The Brown Corpus was a carefully compiled selection of current American English, totalling about a million words drawn from a wide variety of sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.words()[:10] # gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621613"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gutenberg.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783753"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1161192+2621613+len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3783753"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = brown.words() + gutenberg.words() + words # combine the brown and gutenberg corpus\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['of',\n",
       " 'garbage',\n",
       " 'should',\n",
       " 'be',\n",
       " 'cleaned',\n",
       " 'immediately',\n",
       " 'This',\n",
       " 'is',\n",
       " 'their',\n",
       " 'house']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[-10:] # last 10 words in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84569"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(corpus)) # unique words in corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(string.punctuation)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unwanted_words = list(string.punctuation) + stopwords.words('english') \n",
    "len(unwanted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of']\n"
     ]
    }
   ],
   "source": [
    "# Lowercasing\n",
    "lower = [x.lower() for x in corpus]\n",
    "lower[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', \"atlanta's\", 'recent', 'primary']\n"
     ]
    }
   ],
   "source": [
    "# Punctuation and stop words removal\n",
    "filtered_words = [x for x in lower if x not in unwanted_words]\n",
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1676346"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534110"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_words = list(filter(lambda x: x.isalpha() and len(x)>1, filtered_words))\n",
    "len(filtered_words) # clean corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Bi-Gram Model\n",
    "- N-Gram model is useful in capturing word order related regulations\n",
    "- uses N-1 words of prior context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = ngrams(filtered_words,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday')]\n"
     ]
    }
   ],
   "source": [
    "bigrams = list(bigrams) \n",
    "print(type(bigrams), bigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nltk.ConditionalFreqDist(bigrams) # creates a frequency distribution for each condition (the first item in each pair) and count the number of occurrences of each event (the second item in each pair) given that condition i.e, it will give you the frequency of each word following a given word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.ConditionalFreqDist"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fulton [('county', 6), ('superior', 2), ('legislators', 2), ('taxpayers', 1), ('court', 1)]\n",
      "county [('school', 5), ('jail', 4), ('unit', 3), ('new', 3), ('hospital', 3)]\n",
      "grand [('jury', 11), ('old', 4), ('champion', 3), ('pianoforte', 3), ('foe', 3)]\n",
      "jury [('said', 10), ('judge', 4), ('box', 4), ('recommended', 2), ('room', 2)]\n",
      "said [('unto', 1698), ('lord', 146), ('mr', 132), ('alice', 123), ('turnbull', 121)]\n"
     ]
    }
   ],
   "source": [
    "for condition in itertools.islice(model.conditions(), 5):  # Only print for the first 10 conditions\n",
    "    print(condition, model[condition].most_common(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Mr Patrick is our new MASK.\",\"The company MASK all the terms.\",\"Please don't keep your dog on the MASK.\",\"The MASK is my best friend.\",\"I need some MASK products for my craftwork.\",\"The actor MASK the Oscar.\",\"I will call you MASK in the evening.\",\"Covid MASK the lungs.\",\"The MASK of the ministers were sworn in yesterday.\",\"Robert MASK wants to accompany us to the park.\",\"Mia will MASK me about choosing fashion as my career.\", \"The MASK at the zoo was very playful.\",\"The sheep have a lot of MASK that keeps them warm.\",\"The hot spring is at the MASK corner of the street.\",\"Can you MASK me on how to study for exams.\",\"The team will MASK the match if they don't play well.\",\"Can you go MASK the market for me.\",\"The teachers asked the students to keep MASK.\", \"The MASK of garbage should be cleaned immediately.\",\"This is MASK house.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = [['principle','principal'],['accepted','excepted'],['lose','loose'],['later','latter'],['stationary','stationery'],['accepted','excepted'],['later','latter'],['affects','effects'],['council','counsel'],['too','to'],['council','counsel'],['bear','bare'],['fur','fir'],['furthest','farthest'],['advice','advise'],['loose','lose'],['to','for'],['quiet','quite'],['heep','heap'],['there','their']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(question, model, choice1, choice2):\n",
    "    processed = [i.lower() for i in question.split() if i not in unwanted_words]\n",
    "    for i in processed:\n",
    "        if i in [\"mask\", \"mask.\"]:\n",
    "            break\n",
    "        prev = i\n",
    "    prob1 = model[prev].freq(choice1)\n",
    "    prob2 = model[prev].freq(choice2)\n",
    "    print('--------------------------------------------------')\n",
    "    print(f\"{choice1} = {prob1:.4f}\\n{choice2} = {prob2:.4f}\")\n",
    "    if prob1 > prob2:\n",
    "        return question.replace(\"MASK\",choice1.upper())\n",
    "    return question.replace(\"MASK\",choice2.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "principle = 0.0012\n",
      "principal = 0.0016\n",
      "1. Mr Patrick is our new PRINCIPAL.\n",
      "--------------------------------------------------\n",
      "accepted = 0.0066\n",
      "excepted = 0.0033\n",
      "2. The company ACCEPTED all the terms.\n",
      "--------------------------------------------------\n",
      "lose = 0.0103\n",
      "loose = 0.0205\n",
      "3. Please don't keep your dog on the LOOSE.\n",
      "--------------------------------------------------\n",
      "later = 0.0000\n",
      "latter = 0.0000\n",
      "4. The LATTER is my best friend.\n",
      "--------------------------------------------------\n",
      "stationary = 0.0031\n",
      "stationery = 0.0062\n",
      "5. I need some STATIONERY products for my craftwork.\n",
      "--------------------------------------------------\n",
      "accepted = 0.0678\n",
      "excepted = 0.0339\n",
      "6. The actor ACCEPTED the Oscar.\n",
      "--------------------------------------------------\n",
      "later = 0.0071\n",
      "latter = 0.0000\n",
      "7. I will call you LATER in the evening.\n",
      "--------------------------------------------------\n",
      "affects = 1.0000\n",
      "effects = 0.0000\n",
      "8. Covid AFFECTS the lungs.\n",
      "--------------------------------------------------\n",
      "council = 0.0000\n",
      "counsel = 0.0000\n",
      "9. The COUNSEL of the ministers were sworn in yesterday.\n",
      "--------------------------------------------------\n",
      "too = 0.0000\n",
      "to = 0.0000\n",
      "10. Robert TO wants to accompany us to the park.\n",
      "--------------------------------------------------\n",
      "council = 0.3333\n",
      "counsel = 0.6667\n",
      "11. Mia will COUNSEL me about choosing fashion as my career.\n",
      "--------------------------------------------------\n",
      "bear = 0.0000\n",
      "bare = 0.0000\n",
      "12. The BARE at the zoo was very playful.\n",
      "--------------------------------------------------\n",
      "fur = 0.0193\n",
      "fir = 0.0000\n",
      "13. The sheep have a lot of FUR that keeps them warm.\n",
      "--------------------------------------------------\n",
      "furthest = 0.0064\n",
      "farthest = 0.0127\n",
      "14. The hot spring is at the FARTHEST corner of the street.\n",
      "--------------------------------------------------\n",
      "advice = 0.0000\n",
      "advise = 0.0000\n",
      "15. Can you ADVISE me on how to study for exams.\n",
      "--------------------------------------------------\n",
      "loose = 0.0222\n",
      "lose = 0.0444\n",
      "16. The team will LOSE the match if they don't play well.\n",
      "--------------------------------------------------\n",
      "to = 0.0000\n",
      "for = 0.0000\n",
      "17. Can you go FOR the market for me.\n",
      "--------------------------------------------------\n",
      "quiet = 0.0080\n",
      "quite = 0.0030\n",
      "18. The teachers asked the students to keep QUIET.\n",
      "--------------------------------------------------\n",
      "heep = 0.0000\n",
      "heap = 0.0000\n",
      "19. The HEAP of garbage should be cleaned immediately.\n",
      "--------------------------------------------------\n",
      "there = 0.0000\n",
      "their = 0.0000\n",
      "20. This is THEIR house.\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(questions)):\n",
    "    print(f\"{i + 1}.\" , predict(questions[i], model, choices[i][0],choices[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferences\n",
    "\n",
    "`1. Lack of Context:` Bigrams only consider adjacent pairs of words, which may not capture the full context of a sentence or document. This limitation can lead to misinterpretation or loss of meaning, especially in complex language constructs.\n",
    "\n",
    "`2. Sparse Data:` In datasets with limited occurrences of specific bigrams, statistical models based on bigrams may suffer from sparse data issues. This can result in unreliable estimates and reduced accuracy, particularly in language tasks with diverse vocabulary.\n",
    "\n",
    "`3. Disregard for Word Order:` Bigrams ignore the ordering of words beyond adjacent pairs. Consequently, they cannot capture dependencies or patterns that span more than two consecutive words, limiting their ability to model complex linguistic structures accurately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
